version: 1
current_focus: "Quantization Protocol and v0.1.1 (Numerical Fidelity)"
tasks:
- id: T001
  title: Validate pxVM accuracy vs numpy reference
  status: completed
  type: validation
  priority: critical
  notes: 'COMPLETED: Created complete validation infrastructure:

    - pxvm/examples/validate_pixellm_accuracy.py

    - pxvm/dev/assembler.py (symbolic DSL → pixels)

    - pxvm/dev/inspector.py (pixels → human-readable)

    - pxvm/debug/constraints.py (validation utilities)


    Result: Structural integrity PASSED, but naive uint8 quantization

    causes saturation (all outputs → 255). This validates the architecture

    and exposes the quantization limitation for T002 to address.


    Tagged: v0.1.0 "Proof of Concept"

    '
- id: T002
  title: Integrate quantization protocol into assembler and interpreter
  status: todo
  type: feature
  priority: critical
  notes: 'Implement per-matrix scale/offset quantization:

    1. Update pxvm/utils/layout.py to reserve 2 pixels for metadata

       - Pixel (1, row): scale (4 bytes)

       - Pixel (2, row): offset (4 bytes)

    2. Update pxvm/dev/assembler.py to:

       - Calculate scale/offset per matrix

       - Pack metadata using pxvm/utils/quantization.py

       - Quantize data using linear_quantize()

    3. Update pxvm/core/interpreter.py to:

       - Unpack scale/offset from metadata pixels

       - Dequantize before MATMUL operations

       - Perform MATMUL in float32

       - Requantize output with new scale/offset

    4. Re-run validate_pixellm_accuracy.py

       - Target: correlation > 0.9 with float32 reference


    Blocks: v0.1.1 release

    '
- id: T003
  title: Install wgpu-py and test GPU execution
  status: todo
  type: infrastructure
  priority: high
  notes: 'Install wgpu-py dependencies and verify GPU executor works.

    Run pxvm/examples/compare_cpu_gpu.py for all test cases.


    Expected: Byte-identical output between CPU and GPU

    Prerequisite for: Parallel MatMul kernel optimization

    Dependency: T002 (quantization must work first)

    '
- id: T004
  title: Tag v0.1.1 release
  status: todo
  type: release
  priority: high
  notes: 'After T002 quantization validated, tag the milestone:

    - Numerically faithful execution

    - >0.9 correlation with float32 reference

    - Complete development lifecycle

    - Production-ready Pixel-LLM


    This marks: "Neural networks execute as pixels WITH accuracy"

    '
- id: T005
  title: Implement parallel tiled MatMul kernel (WGSL)
  status: todo
  type: acceleration
  priority: medium
  notes: "Replace sequential triple-loop in pxvm/gpu/interpreter.wgsl\nwith parallel\
    \ kernel:\n- One thread per C[m,n] output element\n- Shared memory tiling for\
    \ cache efficiency\n- Workgroup size: 16×16\n\nExpected: 100x+ speedup vs CPU\
    \ naive implementation\nDependency: T002 (accuracy first), T003 (GPU working)\n"
- id: T006
  title: Implement autoregressive text generation
  status: todo
  type: feature
  priority: medium
  notes: 'Create pxvm/examples/generate_text.py that:

    1. Loads pixellm_forward.pxi

    2. Implements sampling loop (argmax or nucleus)

    3. Generates text token-by-token

    4. Detokenizes and prints output


    This proves: User-facing LLM output from pixel program

    Dependency: T002 (accuracy validation)

    '
- id: T007
  title: Encode tokenizer as pixels
  status: todo
  type: design
  priority: low
  notes: "Design pixel encoding for tokenizer vocabulary:\n- Token ID → embedding\
    \ vector mapping\n- Byte-pair encoding merge rules as pixel tables\n- Special\
    \ tokens (BOS, EOS, PAD)\n\nGoal: Entire inference pipeline (tokenize → forward\
    \ → detokenize)\nruns natively in pxVM without external dependencies\n"
- id: T008
  title: Define OP_SNAPSHOT and OP_FORK opcodes
  status: todo
  type: design
  priority: low
  notes: 'Begin designing process management primitives:

    - OP_SNAPSHOT: Save current VRAM state to row N

    - OP_FORK: Duplicate execution context

    - OP_SWITCH: Change active process


    This is the foundation for multi-process pxOS

    '
- id: T009
  title: Design float16/float32 support (backward compatible)
  status: todo
  type: design
  priority: low
  notes: 'How to support higher precision without breaking v1.0.0 protocol:

    - New opcodes: OP_MATMUL_F16, OP_MATMUL_F32?

    - Multi-channel encoding: RGB holds 3 bytes of float?

    - Separate precision flag in header pixel?


    Constraint: Must remain executor-agnostic

    '
log:
- '2024-11-17T08:45Z: pxVM v0.0.3 Complete Neural Toolkit freeze'
- '2024-11-17T09:30Z: Pixel-LLM integration complete (bugfix: row allocation)'
- '2024-11-17T10:00Z: Utilities library proven valuable in practice'
- '2024-11-17T10:15Z: Infinite Concept Engine framework initialized'
- '2025-11-17T02:57:50.362840Z: Engine processed T001'
- '2025-11-17T18:30Z: T001 COMPLETE - Validation infrastructure and v0.1.0 tagged'
- '2025-11-17T18:35Z: Quantization protocol (pxvm/utils/quantization.py) implemented and tested'
